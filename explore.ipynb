{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T15:51:20.552557Z",
     "start_time": "2025-10-27T15:51:20.539032Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:51:20.583786Z",
     "start_time": "2025-10-27T15:51:20.572597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metadata = r'D:\\KEEP\\FPTUni\\DH_FPT\\FPTU\\Syllabuses\\K√¨ 8\\DSP391m\\Dataset\\processed_taco_coco\\annotations\\instances_train2017.json'\n",
    "\n",
    "with open(metadata, 'r') as f:\n",
    "    metadata = json.load(f)"
   ],
   "id": "2273a036538429de",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:51:20.599339Z",
     "start_time": "2025-10-27T15:51:20.590816Z"
    }
   },
   "cell_type": "code",
   "source": "print(metadata.keys())",
   "id": "797c1dceb0775a74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:51:20.614860Z",
     "start_time": "2025-10-27T15:51:20.606853Z"
    }
   },
   "cell_type": "code",
   "source": "print(metadata['categories'])",
   "id": "e020fd8a965b437c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'supercategory': 'Aluminium foil', 'id': 0, 'name': 'Aluminium foil'}, {'supercategory': 'Battery', 'id': 1, 'name': 'Battery'}, {'supercategory': 'Blister pack', 'id': 2, 'name': 'Aluminium blister pack'}, {'supercategory': 'Blister pack', 'id': 3, 'name': 'Carded blister pack'}, {'supercategory': 'Bottle', 'id': 4, 'name': 'Other plastic bottle'}, {'supercategory': 'Bottle', 'id': 5, 'name': 'Clear plastic bottle'}, {'supercategory': 'Bottle', 'id': 6, 'name': 'Glass bottle'}, {'supercategory': 'Bottle cap', 'id': 7, 'name': 'Plastic bottle cap'}, {'supercategory': 'Bottle cap', 'id': 8, 'name': 'Metal bottle cap'}, {'supercategory': 'Broken glass', 'id': 9, 'name': 'Broken glass'}, {'supercategory': 'Can', 'id': 10, 'name': 'Food Can'}, {'supercategory': 'Can', 'id': 11, 'name': 'Aerosol'}, {'supercategory': 'Can', 'id': 12, 'name': 'Drink can'}, {'supercategory': 'Carton', 'id': 13, 'name': 'Toilet tube'}, {'supercategory': 'Carton', 'id': 14, 'name': 'Other carton'}, {'supercategory': 'Carton', 'id': 15, 'name': 'Egg carton'}, {'supercategory': 'Carton', 'id': 16, 'name': 'Drink carton'}, {'supercategory': 'Carton', 'id': 17, 'name': 'Corrugated carton'}, {'supercategory': 'Carton', 'id': 18, 'name': 'Meal carton'}, {'supercategory': 'Carton', 'id': 19, 'name': 'Pizza box'}, {'supercategory': 'Cup', 'id': 20, 'name': 'Paper cup'}, {'supercategory': 'Cup', 'id': 21, 'name': 'Disposable plastic cup'}, {'supercategory': 'Cup', 'id': 22, 'name': 'Foam cup'}, {'supercategory': 'Cup', 'id': 23, 'name': 'Glass cup'}, {'supercategory': 'Cup', 'id': 24, 'name': 'Other plastic cup'}, {'supercategory': 'Food waste', 'id': 25, 'name': 'Food waste'}, {'supercategory': 'Glass jar', 'id': 26, 'name': 'Glass jar'}, {'supercategory': 'Lid', 'id': 27, 'name': 'Plastic lid'}, {'supercategory': 'Lid', 'id': 28, 'name': 'Metal lid'}, {'supercategory': 'Other plastic', 'id': 29, 'name': 'Other plastic'}, {'supercategory': 'Paper', 'id': 30, 'name': 'Magazine paper'}, {'supercategory': 'Paper', 'id': 31, 'name': 'Tissues'}, {'supercategory': 'Paper', 'id': 32, 'name': 'Wrapping paper'}, {'supercategory': 'Paper', 'id': 33, 'name': 'Normal paper'}, {'supercategory': 'Paper bag', 'id': 34, 'name': 'Paper bag'}, {'supercategory': 'Paper bag', 'id': 35, 'name': 'Plastified paper bag'}, {'supercategory': 'Plastic bag & wrapper', 'id': 36, 'name': 'Plastic film'}, {'supercategory': 'Plastic bag & wrapper', 'id': 37, 'name': 'Six pack rings'}, {'supercategory': 'Plastic bag & wrapper', 'id': 38, 'name': 'Garbage bag'}, {'supercategory': 'Plastic bag & wrapper', 'id': 39, 'name': 'Other plastic wrapper'}, {'supercategory': 'Plastic bag & wrapper', 'id': 40, 'name': 'Single-use carrier bag'}, {'supercategory': 'Plastic bag & wrapper', 'id': 41, 'name': 'Polypropylene bag'}, {'supercategory': 'Plastic bag & wrapper', 'id': 42, 'name': 'Crisp packet'}, {'supercategory': 'Plastic container', 'id': 43, 'name': 'Spread tub'}, {'supercategory': 'Plastic container', 'id': 44, 'name': 'Tupperware'}, {'supercategory': 'Plastic container', 'id': 45, 'name': 'Disposable food container'}, {'supercategory': 'Plastic container', 'id': 46, 'name': 'Foam food container'}, {'supercategory': 'Plastic container', 'id': 47, 'name': 'Other plastic container'}, {'supercategory': 'Plastic glooves', 'id': 48, 'name': 'Plastic glooves'}, {'supercategory': 'Plastic utensils', 'id': 49, 'name': 'Plastic utensils'}, {'supercategory': 'Pop tab', 'id': 50, 'name': 'Pop tab'}, {'supercategory': 'Rope & strings', 'id': 51, 'name': 'Rope & strings'}, {'supercategory': 'Scrap metal', 'id': 52, 'name': 'Scrap metal'}, {'supercategory': 'Shoe', 'id': 53, 'name': 'Shoe'}, {'supercategory': 'Squeezable tube', 'id': 54, 'name': 'Squeezable tube'}, {'supercategory': 'Straw', 'id': 55, 'name': 'Plastic straw'}, {'supercategory': 'Straw', 'id': 56, 'name': 'Paper straw'}, {'supercategory': 'Styrofoam piece', 'id': 57, 'name': 'Styrofoam piece'}, {'supercategory': 'Unlabeled litter', 'id': 58, 'name': 'Unlabeled litter'}, {'supercategory': 'Cigarette', 'id': 59, 'name': 'Cigarette'}]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:24.125902Z",
     "start_time": "2025-10-27T18:45:24.117219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_PATHS = {\n",
    "    \"Rtdetrv2\": \"FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_BASELINE/last.pth\",\n",
    "    \"Distill-Convnet\": \"FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/last.pth\",\n",
    "    \"Distill-Vit\": \"FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth\",\n",
    "    \"YOLOv11l\": \"FINAL/YOLO/yolo11n.pt\"\n",
    "}"
   ],
   "id": "90871378f8a7ee25",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:49:00.381619Z",
     "start_time": "2025-10-27T18:48:59.861941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def inspect_checkpoint(path):\n",
    "    ckpt = torch.load(path, map_location='cpu')\n",
    "\n",
    "    print(f\"\\nüîç Inspecting file: {path}\")\n",
    "    print(f\"Type of object loaded: {type(ckpt)}\")\n",
    "\n",
    "    # Case 1: It's a full model object (serialized)\n",
    "    if not isinstance(ckpt, (dict,)):\n",
    "        print(\"‚Üí This looks like a **full model object** (saved with torch.save(model, ...))\")\n",
    "        print(\"  ‚úÖ You can load directly using: model = torch.load(path)\")\n",
    "        return\n",
    "\n",
    "    # Case 2: It's a dictionary (most common case)\n",
    "    print(\"‚Üí This is a **dictionary checkpoint**. Keys:\")\n",
    "    for key in ckpt.keys():\n",
    "        print(\"   ‚Ä¢\", key)\n",
    "\n",
    "    # Try to identify its type\n",
    "    if \"model\" in ckpt:\n",
    "        print(\"üß† Detected: checkpoint with 'model' weights (may also include optimizer, epoch info).\")\n",
    "    elif all(k.startswith(\"module.\") or \".\" in k for k in ckpt.keys()):\n",
    "        print(\"ü™∂ Detected: pure state_dict (weights only).\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Unknown structure ‚Äî open specific keys manually to verify contents.\")\n",
    "\n",
    "# Example usage\n",
    "inspect_checkpoint(MODEL_PATHS['Rtdetrv2'])\n"
   ],
   "id": "91f87e6a24e504c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Inspecting file: FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_BASELINE/last.pth\n",
      "Type of object loaded: <class 'dict'>\n",
      "‚Üí This is a **dictionary checkpoint**. Keys:\n",
      "   ‚Ä¢ date\n",
      "   ‚Ä¢ last_epoch\n",
      "   ‚Ä¢ model\n",
      "   ‚Ä¢ criterion\n",
      "   ‚Ä¢ postprocessor\n",
      "   ‚Ä¢ scaler\n",
      "   ‚Ä¢ optimizer\n",
      "   ‚Ä¢ lr_scheduler\n",
      "üß† Detected: checkpoint with 'model' weights (may also include optimizer, epoch info).\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%writefile trainer_convnext.py\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import datetime\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "class HuggingFaceTeacherWrapper(nn.Module):\n",
    "    def __init__(self, model_id: str, token: str = None):\n",
    "        super().__init__()\n",
    "        if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "            print(f\"Loading teacher model '{model_id}' from Hugging Face...\")\n",
    "        config = AutoConfig.from_pretrained(model_id, token=token)\n",
    "        self._model = AutoModel.from_pretrained(model_id, token=token)\n",
    "        self.is_vit = \"vit\" in config.model_type.lower()\n",
    "        self._feature_dim = (\n",
    "            self._model.config.hidden_size\n",
    "            if self.is_vit\n",
    "            else self._model.config.hidden_sizes[-1]\n",
    "        )\n",
    "        if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "            print(f\"Detected {'ViT' if self.is_vit else 'ConvNeXT'} architecture. Feature dim: {self._feature_dim}\")\n",
    "\n",
    "    def feature_dim(self) -> int:\n",
    "        return self._feature_dim\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._model(pixel_values=x, output_hidden_states=True)\n",
    "        if self.is_vit:\n",
    "            patch_tokens = outputs.last_hidden_state[:, 1:, :]\n",
    "            b, s, d = patch_tokens.shape\n",
    "            h = w = int(math.sqrt(s))\n",
    "            return patch_tokens.permute(0, 2, 1).reshape(b, d, h, w)\n",
    "        return outputs.hidden_states[-1]\n",
    "\n",
    "class CocoDetectionForDistill(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, ann_file, transforms):\n",
    "        self.root = root\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "        img = Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "        return self.transforms(img), 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def setup_ddp():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "\n",
    "def cleanup_ddp():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def main_training_function(rank, world_size, config):\n",
    "    device = rank\n",
    "\n",
    "    is_main_process = (rank == 0)\n",
    "\n",
    "    if is_main_process:\n",
    "        print(f\"Running DDP on {world_size} GPUs.\")\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        run_name = f\"run_ddp_{timestamp}_lr{config['learning_rate']}_bs{config['batch_size_per_gpu']}\"\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            from huggingface_hub import login\n",
    "            secrets = UserSecretsClient()\n",
    "            hf_token = secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "            wandb_key = secrets.get_secret(\"WANDB_API_KEY\")\n",
    "            login(token=hf_token)\n",
    "            wandb.login(key=wandb_key)\n",
    "            wandb.init(project=config[\"wandb_project\"], config=config, name=run_name)\n",
    "        except Exception:\n",
    "            hf_token = None\n",
    "            print(\"Could not log in, continuing without W&B.\")\n",
    "    else:\n",
    "        hf_token = None\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    teacher_model = HuggingFaceTeacherWrapper(config[\"teacher_hf_id\"], token=hf_token).to(device)\n",
    "    teacher_model.eval()\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"ƒêang t·∫£i student model tr√™n ti·∫øn tr√¨nh ch√≠nh...\")\n",
    "        torch.hub.load(\"lyuwenyu/RT-DETR\", \"rtdetrv2_l\", pretrained=True, trust_repo=True)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    student_hub_model = torch.hub.load(\"lyuwenyu/RT-DETR\", \"rtdetrv2_l\", pretrained=True, trust_repo=True)\n",
    "    student_model = student_hub_model.model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(1, 3, 640, 640).to(device)\n",
    "        student_channels = student_model.encoder(student_model.backbone(x))[-1].shape[1]\n",
    "    teacher_channels = teacher_model.feature_dim()\n",
    "    projection_layer = nn.Conv2d(student_channels, teacher_channels, kernel_size=1).to(device)\n",
    "\n",
    "    student_model = DDP(student_model, device_ids=[device])\n",
    "    projection_layer = DDP(projection_layer, device_ids=[device])\n",
    "\n",
    "    transforms = T.Compose([\n",
    "        T.Resize((640, 640)), T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CocoDetectionForDistill(\n",
    "        root=os.path.join(config[\"dataset_dir\"], \"train2017\"),\n",
    "        ann_file=os.path.join(config[\"dataset_dir\"], \"annotations/instances_train2017.json\"),\n",
    "        transforms=transforms\n",
    "    )\n",
    "    val_dataset = CocoDetectionForDistill(\n",
    "        root=os.path.join(config[\"dataset_dir\"], \"val2017\"),\n",
    "        ann_file=os.path.join(config[\"dataset_dir\"], \"annotations/instances_val2017.json\"),\n",
    "        transforms=transforms\n",
    "    )\n",
    "    if is_main_process:\n",
    "        print(f\"Data loaded: {len(train_dataset)} training images, {len(val_dataset)} validation images.\")\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config[\"batch_size_per_gpu\"],\n",
    "        shuffle=False, num_workers=config[\"num_workers\"], pin_memory=True, drop_last=True, sampler=train_sampler\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=config[\"batch_size_per_gpu\"],\n",
    "        shuffle=False, num_workers=config[\"num_workers\"], pin_memory=True, drop_last=False, sampler=val_sampler\n",
    "    )\n",
    "\n",
    "    params = list(student_model.module.backbone.parameters()) + \\\n",
    "             list(student_model.module.encoder.parameters()) + \\\n",
    "             list(projection_layer.module.parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params, lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=config['scheduler_factor'], patience=config['scheduler_patience'], verbose=is_main_process)\n",
    "\n",
    "    if is_main_process and wandb.run:\n",
    "        wandb.watch((student_model, projection_layer), log=\"all\", log_freq=100)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        start = time.time()\n",
    "        student_model.train()\n",
    "        projection_layer.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Train]\") if is_main_process else train_loader\n",
    "\n",
    "        for images, _ in train_iterator:\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "                teacher_features = teacher_model(images)\n",
    "            student_features = student_model.module.encoder(student_model.module.backbone(images))[-1]\n",
    "            projected = projection_layer(student_features)\n",
    "            teacher_resized = F.interpolate(teacher_features, size=projected.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            loss = criterion(projected, teacher_resized)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        train_loss_tensor = torch.tensor(total_train_loss).to(device)\n",
    "        dist.all_reduce(train_loss_tensor, op=dist.ReduceOp.SUM)\n",
    "        avg_train_loss = train_loss_tensor.item() / (len(train_loader) * world_size)\n",
    "\n",
    "        student_model.eval()\n",
    "        projection_layer.eval()\n",
    "        total_val_loss = 0.0\n",
    "\n",
    "        val_iterator = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['epochs']} [Val]\") if is_main_process else val_loader\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_iterator:\n",
    "                images = images.to(device)\n",
    "                teacher_features = teacher_model(images)\n",
    "                student_features = student_model.module.encoder(student_model.module.backbone(images))[-1]\n",
    "                projected = projection_layer(student_features)\n",
    "                teacher_resized = F.interpolate(teacher_features, size=projected.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                loss = criterion(projected, teacher_resized)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        val_loss_tensor = torch.tensor(total_val_loss).to(device)\n",
    "        dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.SUM)\n",
    "        avg_val_loss = val_loss_tensor.item() / (len(val_loader) * world_size)\n",
    "\n",
    "        if is_main_process:\n",
    "            duration = time.time() - start\n",
    "            print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Duration: {duration:.2f}s\")\n",
    "            if wandb.run:\n",
    "                wandb.log({\"epoch\": epoch + 1, \"train/avg_loss\": avg_train_loss, \"val/avg_loss\": avg_val_loss, \"time/epoch_s\": duration, \"train/epoch_lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stopping_counter = 0\n",
    "                print(f\"Validation loss improved to {best_val_loss:.4f}. Saving best model...\")\n",
    "                best_weights = {**student_model.module.backbone.state_dict(), **student_model.module.encoder.state_dict()}\n",
    "                torch.save({'model': best_weights}, config[\"best_weights_filename\"])\n",
    "\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                print(f\"Validation loss did not improve. Early stopping counter: {early_stopping_counter}/{config['early_stopping_patience']}\")\n",
    "\n",
    "        stop_training = torch.tensor(1 if early_stopping_counter >= config['early_stopping_patience'] else 0, device=device)\n",
    "        dist.all_reduce(stop_training, op=dist.ReduceOp.MAX)\n",
    "        if stop_training.item() == 1:\n",
    "            if is_main_process:\n",
    "                print(\"Early stopping triggered. Training finished.\")\n",
    "            break\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"\\nDistillation finished.\")\n",
    "        final_weights = {**student_model.module.backbone.state_dict(), **student_model.module.encoder.state_dict()}\n",
    "        torch.save({'model': final_weights}, config[\"final_weights_filename\"])\n",
    "        print(f\"Saved final epoch weights to '{config['final_weights_filename']}'\")\n",
    "        print(f\"Best weights were saved to '{config['best_weights_filename']}' with val_loss: {best_val_loss:.4f}\")\n",
    "        if wandb.run:\n",
    "            wandb.summary[\"best_val_loss\"] = best_val_loss\n",
    "            wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_ddp()\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    DATASET_DIR = \"/kaggle/input/dsp-pre-final/processed_taco_coco\"\n",
    "    config = {\n",
    "        \"learning_rate\": 1e-4, \"epochs\": 50, \"batch_size_per_gpu\": 16,\n",
    "        \"num_workers\": 2, \"weight_decay\": 1e-5,\n",
    "        \"teacher_hf_id\": \"facebook/dinov3-convnext-base-pretrain-lvd1689m\",\n",
    "        \"dataset_dir\": DATASET_DIR,\n",
    "        \"scheduler_patience\": 3, \"scheduler_factor\": 0.1,\n",
    "        \"early_stopping_patience\": 7,\n",
    "        \"best_weights_filename\": \"distilled_rtdetr_convnext_teacher_BEST.pth\",\n",
    "        \"final_weights_filename\": \"distilled_rtdetr_convnext_teacher_FINAL.pth\",\n",
    "        \"wandb_project\": \"Distill-RTDETR-ConvNeXt-Teacher\",\n",
    "    }\n",
    "\n",
    "    main_training_function(rank, world_size, config)\n",
    "\n",
    "    cleanup_ddp()"
   ],
   "id": "cfa29a5b90a9d700"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%writefile trainer_vit.py\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import torch\n",
    "import wandb\n",
    "import lightly_train\n",
    "from lightly_train.model_wrappers import RTDETRModelWrapper\n",
    "import datetime\n",
    "\n",
    "def main_training_function(config):\n",
    "    is_main_process = os.environ.get(\"LOCAL_RANK\", \"0\") == \"0\"\n",
    "\n",
    "    if is_main_process:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        run_name = f\"run_ddp_{timestamp}_lr{config['learning_rate']}_bs{config['batch_size_per_gpu']}\"\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            wandb_key = UserSecretsClient().get_secret(\"WANDB_API_KEY\")\n",
    "            wandb.login(key=wandb_key)\n",
    "        except Exception as e:\n",
    "            print(f\"W&B secrets not available. Skipping login. Error: {e}\")\n",
    "\n",
    "        if os.path.exists(config['output_dir']):\n",
    "            print(f\"Output directory '{config['output_dir']}' already exists. Deleting it.\")\n",
    "            shutil.rmtree(config['output_dir'])\n",
    "\n",
    "    if not is_main_process:\n",
    "        run_name = \"\"\n",
    "\n",
    "    if torch.distributed.is_initialized():\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"Initializing Student Model (RT-DETR)...\")\n",
    "    student_hub_model = torch.hub.load('lyuwenyu/RT-DETR', 'rtdetrv2_l', pretrained=True, trust_repo=True)\n",
    "    wrapped_student = RTDETRModelWrapper(student_hub_model.model)\n",
    "\n",
    "    callbacks_config = {\n",
    "        \"model_checkpoint\": {\n",
    "            \"dirpath\": os.path.join(config['output_dir'], 'checkpoints'),\n",
    "            \"filename\": 'best-model-{epoch}-{validation_loss:.4f}',\n",
    "            \"monitor\": 'val_loss',\n",
    "            \"mode\": 'min',\n",
    "            \"save_top_k\": 1,\n",
    "        },\n",
    "        \"learning_rate_monitor\": {}\n",
    "    }\n",
    "\n",
    "    global_batch_size = config['batch_size_per_gpu'] * config['num_gpus']\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"Starting distillation with lightly_train.train()...\")\n",
    "        print(f\"Global batch size: {global_batch_size} ({config['batch_size_per_gpu']} per GPU)\")\n",
    "\n",
    "    lightly_train.train(\n",
    "        model=wrapped_student,\n",
    "        method=\"distillationv1\",\n",
    "        method_args={\n",
    "            \"teacher\": config['teacher_name'],\n",
    "            \"teacher_url\": config['teacher_url'],\n",
    "        },\n",
    "        data=[config['train_dir'], config['val_dir']],\n",
    "        out=config['output_dir'],\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=global_batch_size,\n",
    "        num_workers=config['num_workers'],\n",
    "        optim=config['optimizer_name'],\n",
    "        optim_args={\"lr\": config['learning_rate'], \"weight_decay\": config['weight_decay']},\n",
    "        callbacks=callbacks_config,\n",
    "        loggers={\n",
    "            \"wandb\": {\n",
    "                \"project\": config['wandb_project'],\n",
    "                \"name\": run_name,\n",
    "            }\n",
    "        },\n",
    "        devices=config['num_gpus'],\n",
    "        strategy='ddp_find_unused_parameters_true',\n",
    "        accelerator='gpu'\n",
    "    )\n",
    "    if is_main_process:\n",
    "        print(\"\\nDistillation finished.\")\n",
    "        print(f\"Best model checkpoint saved in directory: {os.path.join(config['output_dir'], 'checkpoints')}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DINOV3_VIT_TEACHER_URL = \"\"\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        DINOV3_VIT_TEACHER_URL = UserSecretsClient().get_secret(\"DINOV3_TEACHER_URL\")\n",
    "    except Exception as e:\n",
    "         print(f\"Could not read secret 'DINOV3_VIT_URL'. Please set it manually. Error: {e}\")\n",
    "\n",
    "    if not DINOV3_VIT_TEACHER_URL:\n",
    "        print(\"ERROR: Save your token key into kaggle secret\")\n",
    "    else:\n",
    "        BASE_DIR = \"/kaggle/input/dsp-pre-final/processed_taco_coco\"\n",
    "        TRAIN_DIR = os.path.join(BASE_DIR, \"train2017\")\n",
    "        VAL_DIR = os.path.join(BASE_DIR, \"val2017\")\n",
    "\n",
    "        config = {\n",
    "            \"num_gpus\": 2,\n",
    "            \"epochs\": 50, \"batch_size_per_gpu\": 8, \"num_workers\": 2,\n",
    "            \"optimizer_name\": \"adamw\", \"learning_rate\": 1e-4, \"weight_decay\": 1e-5,\n",
    "            \"early_stopping_patience\": 7,\n",
    "            \"teacher_name\": \"dinov3/vitb16\",\n",
    "            \"teacher_url\": DINOV3_VIT_TEACHER_URL,\n",
    "            \"train_dir\": TRAIN_DIR,\n",
    "            \"val_dir\": VAL_DIR,\n",
    "            \"output_dir\": \"out/distill_vit_lightly\",\n",
    "            \"wandb_project\": \"Distill-RTDETR-Distill-VIT\"\n",
    "        }\n",
    "\n",
    "        main_training_function(config)"
   ],
   "id": "48cd0d57d6a1a31f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%writefile /kaggle/working/RT-DETR/rtdetrv2_pytorch/configs/rtdetrv2/rtdetrv2_taco_finetune_convnext.yml\n",
    "__include__: [\n",
    "  '../dataset/coco_detection.yml',\n",
    "  '../runtime.yml',\n",
    "  './include/dataloader.yml',\n",
    "  './include/rtdetrv2_r50vd.yml',\n",
    "]\n",
    "\n",
    "output_dir: ./output/rtdetrv2_finetune_taco_convnext_teacher\n",
    "\n",
    "RTDETR:\n",
    "  backbone: HGNetv2\n",
    "\n",
    "HGNetv2:\n",
    "  name: 'L'\n",
    "  return_idx: [1, 2, 3]\n",
    "  freeze_at: 0\n",
    "  freeze_norm: True\n",
    "  pretrained: True\n",
    "\n",
    "task: detection\n",
    "remap_mscoco_category: false\n",
    "tuning: '../distilled_rtdetr_convnext_teacher_BEST.pth'\n",
    "compile: true\n",
    "epoches: 50\n",
    "\n",
    "num_classes: 60\n",
    "\n",
    "train_dataloader:\n",
    "  num_workers: 4\n",
    "  dataset:\n",
    "    type: CocoDetection\n",
    "    img_folder: /kaggle/input/dsp-pre-final/processed_taco_coco/train2017\n",
    "    ann_file: /kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_train2017.json\n",
    "\n",
    "val_dataloader:\n",
    "  num_workers: 4\n",
    "  dataset:\n",
    "    type: CocoDetection\n",
    "    img_folder: /kaggle/input/dsp-pre-final/processed_taco_coco/val2017\n",
    "    ann_file: /kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_val2017.json\n",
    "\n",
    "batch_size: 16\n",
    "\n",
    "optimizer:\n",
    "  type: AdamW\n",
    "  params:\n",
    "    - params: '^(?=.*backbone)'\n",
    "      lr: 0.00001\n",
    "  lr: 0.0001\n",
    "  weight_decay: 0.0001\n",
    "  betas: [0.9, 0.999]\n",
    "\n",
    "lr_scheduler:\n",
    "  type: OneCycleLR\n",
    "  max_lr: 0.0001\n",
    "  pct_start: 0.3\n",
    "  total_steps: 4000\n",
    "\n",
    "checkpoint_freq: 10"
   ],
   "id": "e794c7ad17ea6e03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T19:53:52.058958Z",
     "start_time": "2025-11-02T19:53:51.145223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from camera_utils import manual_roi_setup, camera_stream, draw_rois\n",
    "\n",
    "def test_camera_utils():\n",
    "    print(\"========== CAMERA UTILS TEST ==========\")\n",
    "    print(\"[1] Starting ROI setup...\")\n",
    "\n",
    "    # Step 1: Draw the ROIs manually\n",
    "    rois = manual_roi_setup(cam_index=0, fps_limit=30)\n",
    "\n",
    "    if not rois:\n",
    "        print(\"[WARN] No ROIs defined. Exiting test.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] {len(rois)} ROIs finalized:\")\n",
    "    for roi in rois:\n",
    "        print(f\"   - {roi['label']}: ({roi['x']},{roi['y']},{roi['w']},{roi['h']})\")\n",
    "\n",
    "    # Step 2: Open camera stream to preview the ROIs\n",
    "    print(\"[2] Starting camera stream with ROI overlay (press 'q' to exit)...\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Cannot open webcam.\")\n",
    "        return\n",
    "\n",
    "    fps_limit = 30\n",
    "    frame_time = 1.0 / fps_limit\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Draw the predefined ROIs on the frame\n",
    "        draw_rois(frame, rois)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Camera ROI Test\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "\n",
    "        # Limit FPS\n",
    "        cv2.waitKey(int(1000 / fps_limit))\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"[INFO] Test completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_camera_utils()\n"
   ],
   "id": "204a2d4fbd1a225c",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'camera_utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcv2\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcamera_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m manual_roi_setup, camera_stream, draw_rois\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtest_camera_utils\u001B[39m():\n\u001B[32m      5\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m========== CAMERA UTILS TEST ==========\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'camera_utils'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T02:55:57.993972Z",
     "start_time": "2025-11-03T02:55:56.249059Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "441a93c8a3b4cb7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  3 09:55:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.57                 Driver Version: 581.57         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P0             14W /   80W |       0MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T02:58:11.180552Z",
     "start_time": "2025-11-03T02:58:11.174196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"True\" if torch.cuda.is_available() else \"False\")"
   ],
   "id": "2a71d5cc54dde60d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed82cf667512b8e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
